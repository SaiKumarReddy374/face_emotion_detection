{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hETaLFQv-UuW",
        "outputId": "9dcb8d0b-308d-4b56-c219-914c4fb9ae0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 144ms/step - accuracy: 0.1803 - loss: 0.3579 - val_accuracy: 0.2420 - val_loss: 0.3172 - learning_rate: 1.0000e-04\n",
            "Epoch 2/30\n",
            "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 146ms/step - accuracy: 0.2455 - loss: 0.3193 - val_accuracy: 0.2934 - val_loss: 0.3136 - learning_rate: 1.0000e-04\n",
            "Epoch 3/30\n",
            "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 143ms/step - accuracy: 0.3175 - loss: 0.2946 - val_accuracy: 0.4508 - val_loss: 0.2345 - learning_rate: 1.0000e-04\n",
            "Epoch 4/30\n",
            "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 148ms/step - accuracy: 0.4222 - loss: 0.2461 - val_accuracy: 0.5092 - val_loss: 0.2064 - learning_rate: 1.0000e-04\n",
            "Epoch 5/30\n",
            "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 151ms/step - accuracy: 0.4859 - loss: 0.2171 - val_accuracy: 0.5480 - val_loss: 0.1810 - learning_rate: 1.0000e-04\n",
            "Epoch 6/30\n",
            "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 133ms/step - accuracy: 0.5225 - loss: 0.1975 - val_accuracy: 0.5649 - val_loss: 0.1705 - learning_rate: 1.0000e-04\n",
            "Epoch 7/30\n",
            "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 134ms/step - accuracy: 0.5483 - loss: 0.1814 - val_accuracy: 0.5977 - val_loss: 0.1561 - learning_rate: 1.0000e-04\n",
            "Epoch 8/30\n",
            "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 146ms/step - accuracy: 0.5762 - loss: 0.1673 - val_accuracy: 0.6298 - val_loss: 0.1450 - learning_rate: 1.0000e-04\n",
            "Epoch 9/30\n",
            "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 133ms/step - accuracy: 0.5936 - loss: 0.1586 - val_accuracy: 0.6343 - val_loss: 0.1408 - learning_rate: 1.0000e-04\n",
            "Epoch 10/30\n",
            "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 149ms/step - accuracy: 0.6173 - loss: 0.1502 - val_accuracy: 0.6531 - val_loss: 0.1332 - learning_rate: 1.0000e-04\n",
            "Epoch 11/30\n",
            "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 133ms/step - accuracy: 0.6336 - loss: 0.1413 - val_accuracy: 0.6546 - val_loss: 0.1327 - learning_rate: 1.0000e-04\n",
            "Epoch 12/30\n",
            "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 146ms/step - accuracy: 0.6463 - loss: 0.1341 - val_accuracy: 0.6638 - val_loss: 0.1254 - learning_rate: 1.0000e-04\n",
            "Epoch 13/30\n",
            "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 134ms/step - accuracy: 0.6539 - loss: 0.1297 - val_accuracy: 0.6625 - val_loss: 0.1268 - learning_rate: 1.0000e-04\n",
            "Epoch 14/30\n",
            "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 135ms/step - accuracy: 0.6702 - loss: 0.1225 - val_accuracy: 0.6731 - val_loss: 0.1237 - learning_rate: 1.0000e-04\n",
            "Epoch 15/30\n",
            "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 148ms/step - accuracy: 0.6838 - loss: 0.1180 - val_accuracy: 0.6982 - val_loss: 0.1151 - learning_rate: 1.0000e-04\n",
            "Epoch 16/30\n",
            "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 144ms/step - accuracy: 0.6952 - loss: 0.1125 - val_accuracy: 0.6838 - val_loss: 0.1193 - learning_rate: 1.0000e-04\n",
            "Epoch 17/30\n",
            "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 135ms/step - accuracy: 0.6995 - loss: 0.1069 - val_accuracy: 0.7050 - val_loss: 0.1126 - learning_rate: 1.0000e-04\n",
            "Epoch 18/30\n",
            "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 134ms/step - accuracy: 0.7193 - loss: 0.1008 - val_accuracy: 0.7132 - val_loss: 0.1096 - learning_rate: 1.0000e-04\n",
            "Epoch 19/30\n",
            "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 149ms/step - accuracy: 0.7301 - loss: 0.0970 - val_accuracy: 0.7192 - val_loss: 0.1091 - learning_rate: 1.0000e-04\n",
            "Epoch 20/30\n",
            "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 149ms/step - accuracy: 0.7372 - loss: 0.0924 - val_accuracy: 0.7209 - val_loss: 0.1092 - learning_rate: 1.0000e-04\n",
            "Epoch 21/30\n",
            "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 135ms/step - accuracy: 0.7485 - loss: 0.0876 - val_accuracy: 0.7330 - val_loss: 0.1046 - learning_rate: 1.0000e-04\n",
            "Epoch 22/30\n",
            "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 135ms/step - accuracy: 0.7596 - loss: 0.0823 - val_accuracy: 0.7298 - val_loss: 0.1069 - learning_rate: 1.0000e-04\n",
            "Epoch 23/30\n",
            "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 147ms/step - accuracy: 0.7680 - loss: 0.0802 - val_accuracy: 0.7343 - val_loss: 0.1063 - learning_rate: 1.0000e-04\n",
            "Epoch 24/30\n",
            "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 148ms/step - accuracy: 0.7939 - loss: 0.0693 - val_accuracy: 0.7606 - val_loss: 0.1021 - learning_rate: 5.0000e-05\n",
            "Epoch 25/30\n",
            "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 134ms/step - accuracy: 0.8071 - loss: 0.0631 - val_accuracy: 0.7613 - val_loss: 0.1003 - learning_rate: 5.0000e-05\n",
            "Epoch 26/30\n",
            "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 150ms/step - accuracy: 0.8202 - loss: 0.0599 - val_accuracy: 0.7629 - val_loss: 0.1029 - learning_rate: 5.0000e-05\n",
            "Epoch 27/30\n",
            "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 148ms/step - accuracy: 0.8259 - loss: 0.0562 - val_accuracy: 0.7544 - val_loss: 0.1067 - learning_rate: 5.0000e-05\n",
            "Epoch 28/30\n",
            "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 133ms/step - accuracy: 0.8354 - loss: 0.0526 - val_accuracy: 0.7755 - val_loss: 0.0996 - learning_rate: 2.5000e-05\n",
            "Epoch 29/30\n",
            "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 146ms/step - accuracy: 0.8450 - loss: 0.0479 - val_accuracy: 0.7791 - val_loss: 0.0999 - learning_rate: 2.5000e-05\n",
            "Epoch 30/30\n",
            "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 146ms/step - accuracy: 0.8512 - loss: 0.0465 - val_accuracy: 0.7866 - val_loss: 0.1003 - learning_rate: 2.5000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved successfully!\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Define constants\n",
        "IMG_SIZE = (48, 48)\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 30\n",
        "NUM_CLASSES = 7\n",
        "\n",
        "# Load FER-2013 dataset\n",
        "df = pd.read_csv('fer2013.csv')\n",
        "\n",
        "# Process images and labels\n",
        "X, y = [], []\n",
        "for _, row in df.iterrows():\n",
        "    pixels = np.array(row['pixels'].split(), dtype='float32').reshape(48, 48)\n",
        "    X.append(np.stack((pixels,) * 3, axis=-1))  # Convert grayscale to RGB\n",
        "    y.append(row['emotion'])\n",
        "\n",
        "X = np.array(X) / 255.0  # Normalize images\n",
        "y = np.array(y)\n",
        "\n",
        "# Oversample minority classes\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "ros = RandomOverSampler(sampling_strategy='auto', random_state=42)\n",
        "X, y = ros.fit_resample(X.reshape(X.shape[0], -1), y)  # Reshape to 2D for oversampling\n",
        "X = X.reshape(-1, 48, 48, 3)  # Reshape back\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y = to_categorical(y, num_classes=NUM_CLASSES)\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Data Augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2\n",
        ")\n",
        "train_generator = datagen.flow(X_train, y_train, batch_size=BATCH_SIZE)\n",
        "\n",
        "# Load VGG16 pre-trained model\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(48, 48, 3))\n",
        "\n",
        "# Fine-tune last 6 layers\n",
        "for layer in base_model.layers[:-6]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add custom classification head\n",
        "x = Flatten()(base_model.output)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "out = Dense(NUM_CLASSES, activation='softmax', dtype='float32')(x)\n",
        "\n",
        "# Create model\n",
        "model = Model(inputs=base_model.input, outputs=out)\n",
        "\n",
        "# Define Focal Loss\n",
        "def focal_loss(alpha=0.25, gamma=2.0):\n",
        "    def loss(y_true, y_pred):\n",
        "        y_pred = tf.keras.backend.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "        loss = -y_true * (alpha * tf.keras.backend.pow(1 - y_pred, gamma) * tf.keras.backend.log(y_pred))\n",
        "        return tf.keras.backend.sum(loss, axis=1)\n",
        "    return loss\n",
        "\n",
        "# Compile model with Focal Loss\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss=focal_loss(), metrics=['accuracy'])\n",
        "\n",
        "# Callbacks\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6)\n",
        "\n",
        "# Train model\n",
        "model.fit(train_generator, validation_data=(X_val, y_val), epochs=EPOCHS, callbacks=[reduce_lr])\n",
        "\n",
        "# Save model\n",
        "model.save('fer_vgg16_oversampled_focal_loss.h5')\n",
        "print(\"Model saved successfully!\")\n"
      ]
    }
  ]
}